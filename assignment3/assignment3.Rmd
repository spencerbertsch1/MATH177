---
title: "Assignment 3 - QBS 177"
output: html_notebook
---

Spencer Bertsch  
Jan. 2022

------------

Some helpful commands:  
* Insert a new code chunk: *Cmd+Option+I*  
* Run a code cell: *Cmd+Shift+Enter*  
* Preview the notebook HTML file: *Cmd+Shift+K*  


### Imports 
```{r}
library(glue)
```

First we need to change our current working directory to the correct directory for Assignment 3
```{r}
setwd('/Users/spencerbertsch/Desktop/dev/phd_courses/MATH177/assignment3')
getwd()
```

### Import the raw data 
```{r}
lung_df <- read.table("PCA.example1.txt",header=TRUE)
```


Here we examine the mean values of the columns. If the mean is NULL, that means there are NULL values in the dataframe that we need to fill with a method such as forward filling or mean value replacement.
```{r}
mu = colMeans(lung_df)
print(mu[0:5])
```



We can see from the above values of mu that there are NULL values in the dataframe. We need to fill thosw null values using a method such as forward filling or mean value replacement. 

```{r}
for(i in 1:ncol(lung_df)){
  lung_df[is.na(lung_df[,i]), i] <- round(mean(lung_df[,i], na.rm = TRUE))
}
sum(is.na(lung_df))
```

We can see from the 'mu_new' variable that we have successfully removed all of the NULL values from the columns. Let's continue. 
```{r}
mu_new = colMeans(lung_df)
print(mu_new[0:5])
```


### PCA using Spectral Decomposition
```{r}
start.time <- Sys.time()
# here we can perform PCA on the lung cancer dataset using spectral composition
lung.pc.eigen.cor <- prcomp(lung_df)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```


We can now identify the principal components in our new dataframe
```{r}
names(lung.pc.eigen.cor)
lung.scores.eigen <- lung.pc.eigen.cor$x
lung.sdev.eigen <- lung.pc.eigen.cor$sdev
lung.rotation.eigen <- lung.pc.eigen.cor$rotation
lung.center.eigen <- lung.pc.eigen.cor$center
lung.scale.eigen <- lung.pc.eigen.cor$scale
```


#### We can see that the result of the prcomp() function is an array that contains several things: 
1. The standard deviations of the principal components. This is a vector with dimensions **1x(# variables)**
2. The rotations (loadings) of each variable. These are the matrix with dimensions **(# variables)x(# variables)**, each column corresponds to an eigenvector
3. The center is a vector displaying the centers of each column 
4. The scale, a boolean value showing whether or not centering and scaling was used. **False** in our case here. 
5. The x here refers to the rotated data. This is generated by taking the matrix product of the centered data and the rotation matrix described in (2) above. The dimensions of 'x' are **(# observations)x(# variables)**

\br
Knowing all of this, we can now take our rotation (loadings) and perform a matrix product with the initial, un-centered data set that we imported directly from the flat file. The result of this operation should be equal to 'x' above, or our rotated data. We can test this by examining the correlation between these values and the values of the rotated data pull directly from `lung.pc.eigen.cor$x`.
```{r}
pc1 <- (lung.pc.eigen.cor$rotation[,1]%*%t(lung_df))
cor(t(pc1), lung.scores.eigen[,1])
```

And indeed, we see that we get a correlation of 1. 

We can now work backwards and reconstruct the covariance matrix using the rotations (loadings) and the eigenvalues that were generated using the `prcomp` command. 

```{r}
# first we get the variance of all the principal components by taking the square of the standard deviation vector
lung.variance <- lung.pc.eigen.cor$sdev^2

# then we take our vector of length (# variables) and we use it to create a diagonal matrix (here the vector is the main diagonal)
eigenvalue <- diag(lung.variance)

# then we multiply the rotation matrix 
newdata = lung.rotation.eigen%*%eigenvalue%*%t(lung.rotation.eigen)
```


And now we can check our work by comparing the resulting dataframe `newdata` against the actual covariance matrix of the initial lung_df dataframe. 
```{r}
covdata <- cov(lung_df)
sum(abs(newdata-covdata))
```

And indeed we see a value that's very close to zero. We can approximate this as zero and assume that we have successfully reconstructed the covariance matrix by performing the following: 

$COV = rotations \times diagonal\_variance \times rotations^T$


### Use Singular Value Decomposition to perform PCA

In addition to using the `prcomp` function to perform PCA, we can also use singular value decomposition achieve the same results. 
```{r}
start.time <- Sys.time()

# scale our raw lung_df dataframe
lung_df_scaled <- scale(lung_df, scale = FALSE)

# use the 'svd' method to perform singular value decomposition on our newly scaled dataframe 
lung.pc.scale.svd <- svd(lung_df_scaled);

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

#### Runtime for Spectral Decomposition and SVD: 
We can see that the singular value decomposition took **3.989584 mins**, while the PCA performed using spectral decomposition via the `prcomp` command took **4.649158 mins**. Spectral demposition took **roughly 15% longer** to complete than the SVD operation. 


\n
We can now examine the variance for the first 15 principle components: 
```{r}
screeplot(lung.pc.eigen.cor, type = "l", npcs = 15, main = "Screeplot of the first 15 PCs - Lung Data")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
col=c("red"), lty=5, cex=0.6)
```

We can see that the variance in the first three principle components are much higher than the rest, but that doesn't necessarily mean that they contain the majority of the variance for the entire matrix. We have so many (over 1,000) components that even small amount of explained variance in the smaller components can add up. 

\n 
We can now plot the cumulative variance for the same first 15 principle components: 

```{r}
cumpro <- cumsum(lung.pc.eigen.cor$sdev^2 / sum(lung.pc.eigen.cor $sdev^2))
plot(cumpro[0:15], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
```

And here indeed we can see that although there are much larger amounts of explained variance in the first three components, the cumulative explained variance for all three components is only ~0.038, or 3.8% of the explained variance. This just goes to show that although PCA can find the principle components that explain the most variance, you are often still losing a lot of information from the original matrix if you only use a handful of principle components. 


\n 
At this point we can plot the first two principle components against one another to visualize them: 
```{r}
plot(lung.scores.eigen[,1], lung.scores.eigen[,2], xlab="PCA 1", ylab="PCA 2")
```

We know that the last 505 samples include demographic information for three different populations (Europe, Africa, and Asia). We can extract and isolate the components representing each different population here: 

```{r}
plot(lung.scores.eigen[1:2830,1], lung.scores.eigen[1:2830,2],
     type="p",col="black",pch=1,xlab="PC1", ylab="PC2", 
     xlim = range(lung.scores.eigen[,1]), 
     ylim = range(lung.scores.eigen[,2]));

points(lung.scores.eigen[19662:19826,1], lung.scores.eigen[19662:19826,2], col="red",pch=20);
points(lung.scores.eigen[19827:19963,1], lung.scores.eigen[19827:19963,2], col="green",pch=20)
points(lung.scores.eigen[19964:20166,1], lung.scores.eigen[19964:20166,2], col="blue",pch=20)
title(main="Principal Components Analysis using princomp in R", col.main="black", font.main=1,outer=T)
```

Now that we see our clusters, we can find the centroids of each one by taking the mean of only the data in that cluster: 

```{r}
lung.euro.centroid <- apply(lung.scores.eigen[19662:19826,1:2], 2, mean)
lung.asian.centroid <- apply(lung.scores.eigen[19827:19963,1:2], 2, mean)
lung.african.centroid <- apply(lung.scores.eigen[19964:20166,1:2],2,mean)

print('The centroid for the European region is:')
print(lung.euro.centroid)
print('The centroid for the Asian region is:')
print(lung.asian.centroid)
print('The centroid for the African region is:')
print(lung.african.centroid)
```

Now that we have our cluster centroids, we can associate each of our remaining 19,661 samples with its closest centroid based on the distance to each cluster. 

Define a helper function that calculates the euclidean distance given two points
```{r}
find.distance = function(p1, p2) {
  x1 = p1[1]
  y1 = p1[2]
  x2 = p2[1]
  y2 = p2[2]
  
  d = sqrt( ((x2 - x1)^2) + ((y2 - y1)^2) )
  return(d)
}
```



```{r}
cluster.data = function(point, euro.centroid, asian.centroid, african.centroid) {
  print(point)
  print(euro.centroid)
  print(asian.centroid)
  print(african.centroid)
  
  # determine Euclidean distance between the point and each cluster centroid 
  d = find.distance(p1=point, p2=euro.centroid)
  print(glue('Euro Distance: {d}'))
  # return an encoded cluster 
  
  
}
```

```{r}
remaining_points <- lung.scores.eigen[0:19661,1:2]

print(cluster.data(point = remaining_points[1,], 
                   euro.centroid = lung.euro.centroid,
                   asian.centroid = lung.asian.centroid,
                   african.centroid = lung.african.centroid))
```


